{"cells":[{"cell_type":"markdown","metadata":{"id":"biI2UMXHon5o"},"source":["# DNN 구현(3)"]},{"cell_type":"markdown","metadata":{"id":"zOtbGaE_ojQA"},"source":["## 실습 개요\n","\n","1) **실습 목적**\n","\n","이번 실습은 이론으로 배웠던 PyTorch를 이용하여 자연어 처리 데이터셋을 직접 구축하고, 이를 이용하여 다음 단어를 예측하는 모델을 만들어봅니다.\n","\n","\n","2) **수강 목표**\n","\n","- PyTorch를 사용하여 Custom Dataset을 구축할 수 있다.\n","- 자연어 처리를 위한 모델을 구축할 때의 데이터 전처리 방법을 배울 수 있다.\n","- DNN으로 자연어 처리 모델을 구축할 수 있다."]},{"cell_type":"markdown","metadata":{"id":"vUnGWX59ybEk"},"source":["### 실습 목차\n","* 1. Custom Dataset 구축하기\n","  * 1-1. 자연어 데이터의 전처리\n","  * 1-2. 데이터셋 클래스 구축하기\n","\n","* 2. Next word prediction 모델 구축\n","  * 2-1. Next word prediction을 위한 DNN 모델 구축\n","  * 2-2. 모델 학습 및 추론\n"]},{"cell_type":"markdown","metadata":{"id":"3F2fMLWA31Ft"},"source":["### 환경 설정\n","\n","- 패키지 설치 및 임포트"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24494,"status":"ok","timestamp":1692592348488,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"_wvClBdFmDml","outputId":"46393e01-d12d-481c-f661-8ed70cf48cad"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.11/site-packages/PyBioMed-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# !pip install scikit-learn==1.3.0 -q\n","# !pip install torch==2.0.1 -q\n","# !pip install torchvision==0.15.2 -q\n","!pip install torchtext==0.15.2 -q"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GKbkjkhbiPNt","tags":[]},"outputs":[],"source":["import numpy as np # 기본적인 연산을 위한 라이브러리\n","import matplotlib.pyplot as plt # 그림이나 그래프를 그리기 위한 라이브러리\n","from tqdm.notebook import tqdm # 상태 바를 나타내기 위한 라이브러리\n","import pandas as pd # 데이터프레임을 조작하기 위한 라이브러리\n","\n","import torch # PyTorch 라이브러리\n","import torch.nn as nn # 모델 구성을 위한 라이브러리\n","import torch.optim as optim # optimizer 설정을 위한 라이브러리\n","from torch.utils.data import Dataset, DataLoader # 데이터셋 설정을 위한 라이브러리\n","\n","from torchtext.data import get_tokenizer # torch에서 tokenizer를 얻기 위한 라이브러리\n","import torchtext # torch에서 text를 더 잘 처리하기 위한 라이브러리\n","\n","from sklearn.metrics import accuracy_score # 성능지표 측정\n","from sklearn.model_selection import train_test_split # train-validation-test set 나누는 라이브러리\n","\n","import re # text 전처리를 위한 라이브러리"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qnIkzdqds7t4","tags":[]},"outputs":[],"source":["# seed 고정\n","import random\n","import torch.backends.cudnn as cudnn\n","\n","def random_seed(seed_num):\n","    torch.manual_seed(seed_num)\n","    torch.cuda.manual_seed(seed_num)\n","    torch.cuda.manual_seed_all(seed_num)\n","    np.random.seed(seed_num)\n","    cudnn.benchmark = False\n","    cudnn.deterministic = True\n","    random.seed(seed_num)\n","\n","random_seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Bu6Ag-QeD-M5","tags":[]},"outputs":[],"source":["device = 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"expqZQX60qa6"},"source":["###  데이터 셋 개요 </b>\n","\n","* 데이터셋: <a href='https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset'>Medium Dataset</a>\n","* 데이터셋 개요: \"Towards Data Science\", \"UX Collective\", \"The Startup\", \"The Writing Cooperative\", \"Data Driven Investor\", \"Better Humans\", \"Better Marketing\" 의 7개의 주제를 가지는 publication 에 대해서 크롤링을 한 데이터입니다. 원본 데이터는 총 6,508개의 블로그 이미지와 메타 데이터(.csv)로 구성됩니다. 실습에서는 메타데이터를 사용하여 CustomDataset을 구현합니다.\n","  * [How to collect ths dataset?](https://dorianlazar.medium.com/scraping-medium-with-python-beautiful-soup-3314f898bbf5)\n","- 메타 데이터 스키마: 메타 데이터는 총 **10**개의 column으로 구성됩니다.\n","  - id: 아이디\n","  - url: 포스팅 링크\n","  - title: 제목\n","  - subtitle: 부제목\n","  - image: 포스팅 이미지의 파일 이름\n","  - claps: 추천 수\n","  - reponses: 댓글 수\n","  - reading_time: 읽는데 걸리는 시간\n","  - publication: 주제 카테고리(e.g. Towards Data Science..)\n","  - date: 작성 날짜\n","- 데이터 셋 저작권: CC0: Public Domain"]},{"cell_type":"markdown","metadata":{"id":"ga4KpW8DED_Q"},"source":["## 1. Custom Dataset 구축하기\n","\n","```\n","💡 목차 개요: 자연어 데이터를 처리하기 위한 custom dataset class 를 직접 구현해봅니다.\n","```\n","\n","- 1-1. 자연어 데이터의 전처리\n","- 1-2. Custom Dataset class 구축하기\n"]},{"cell_type":"markdown","metadata":{"id":"7a3AnRDwEu4u"},"source":["### 1-1 자연어 데이터 전처리\n","\n","> text로 된 데이터를 어떻게 숫자 형식으로 바꾸고, 모델에 넣는 구조로 바꾸는지 직접 실습해봅니다.\n"]},{"cell_type":"markdown","metadata":{"id":"cu08GdrID-M6"},"source":["#### 📝 설명: Next word prediction\n","* 글의 일부가 주어졌을 때, 다음 단어를 예측 (next word prediction)하는 모델을 구축하는 것을 목표로 합니다.\n","* 예를 들어, \"나는 학교를 가서 밥을 먹었다.\" 라는 문장이 주어진다고 해봅시다.\n","\n","|input|label|\n","|------|---|\n","|나는|학교를|\n","|나는 학교를|가서|\n","|나는 학교를 가서|밥을|\n","|나는 학교를 가서 밥을|먹었다.|\n","\n","* 이와 같이 데이터셋을 구축하고, DNN을 통해 다음 단어를 예측해봅니다.\n","\n","📚 참고할만한 자료:\n","* [Next word prediction](https://wikidocs.net/45101)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"elapsed":303,"status":"ok","timestamp":1692592944318,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"CZ-udSLHrqSl","outputId":"5c39f601-68dc-483a-c0c5-2d452acc3824","tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url</th>\n","      <th>title</th>\n","      <th>subtitle</th>\n","      <th>image</th>\n","      <th>claps</th>\n","      <th>responses</th>\n","      <th>reading_time</th>\n","      <th>publication</th>\n","      <th>date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n","      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n","      <td>NaN</td>\n","      <td>1.png</td>\n","      <td>850</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n","      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n","      <td>NaN</td>\n","      <td>2.png</td>\n","      <td>1100</td>\n","      <td>11</td>\n","      <td>9</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n","      <td>How to Use ggplot2 in Python</td>\n","      <td>A Grammar of Graphics for Python</td>\n","      <td>3.png</td>\n","      <td>767</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>https://towardsdatascience.com/databricks-how-...</td>\n","      <td>Databricks: How to Save Files in CSV on Your L...</td>\n","      <td>When I work on Python projects dealing…</td>\n","      <td>4.jpeg</td>\n","      <td>354</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n","      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n","      <td>One example of building neural…</td>\n","      <td>5.jpeg</td>\n","      <td>211</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                                url  \\\n","0   1  https://towardsdatascience.com/a-beginners-gui...   \n","1   2  https://towardsdatascience.com/hands-on-graph-...   \n","2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n","3   4  https://towardsdatascience.com/databricks-how-...   \n","4   5  https://towardsdatascience.com/a-step-by-step-...   \n","\n","                                               title  \\\n","0  A Beginner’s Guide to Word Embedding with Gens...   \n","1  Hands-on Graph Neural Networks with PyTorch & ...   \n","2                       How to Use ggplot2 in Python   \n","3  Databricks: How to Save Files in CSV on Your L...   \n","4  A Step-by-Step Implementation of Gradient Desc...   \n","\n","                                  subtitle   image  claps responses  \\\n","0                                      NaN   1.png    850         8   \n","1                                      NaN   2.png   1100        11   \n","2         A Grammar of Graphics for Python   3.png    767         1   \n","3  When I work on Python projects dealing…  4.jpeg    354         0   \n","4          One example of building neural…  5.jpeg    211         3   \n","\n","   reading_time           publication        date  \n","0             8  Towards Data Science  2019-05-30  \n","1             9  Towards Data Science  2019-05-30  \n","2             5  Towards Data Science  2019-05-30  \n","3             4  Towards Data Science  2019-05-30  \n","4             4  Towards Data Science  2019-05-30  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data_csv = pd.read_csv('./data/medium_data.csv')\n","data_csv.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GiuA9zb9rBkc","outputId":"21aa133b-50f6-4e0e-ddbe-630a0a5d1959","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(6508, 10)\n"]}],"source":["print(data_csv.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vzRs-fN_sMB3","tags":[]},"outputs":[],"source":["# 각각의 title만 추출합니다.\n","# 우리는 title의 첫 단어가 주어졌을 때, 다음 단어를 예측하는 것을 수행할 것입니다.\n","data = data_csv['title'].values"]},{"cell_type":"markdown","metadata":{"id":"JF2DjmcXD-M7"},"source":["#### 📝 설명: 텍스트 데이터 전처리하기\n","* 해당 데이터셋은 크롤링(인터넷에 있는 정보를 수집하는 기법)을 통해 구축되었기 때문에 no-break space가 종종 발생합니다. 이러한 no-break space를 제거하는 전처리를 진행합니다.\n","  * No-Break Space란? 웹 페이지나 문서 등에서 단어나 문장 사이의 공백이 있는 경우, 해당 공백이 줄 바꿈으로 인해 분리되지 않고 한 단어나 문장으로 인식되도록 하는데 사용되는 공백\n","  * 예시 (no-break-space 사용 X)\n","    ```\n","    Hello\n","    World~\n","    ```\n","    \n","    (no-break-space 사용)\n","    ```\n","    Hello,⎵world!\n","    ```\n","* no-break space를 제거하기 위해선 unicode 형식으로 제거를 해야합니다.\n","  * unicode란? 전 세계의 모든 문자와 기호를 일관성 있게 표현하기 위한 표준 문자 인코딩 체계\n","\n","* `re` 라이브러리를 이용하여 텍스트 데이터를 쉽게 처리할 수 있습니다.\n","\n","📚 참고할만한 자료:\n","* <a href='https://www.compart.com/en/unicode'>unicode 검색 사이트</a>\n","* [re 라이브러리를 이용한 텍스트 데이터 사용법](https://velog.io/@hoegon02/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-12-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%A0%84%EC%B2%98%EB%A6%AC-%EC%A0%95%EA%B7%9C-%ED%91%9C%ED%98%84%EC%8B%9D-3qmtwryf)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"kFmwB6NWD-M7","outputId":"76492532-2031-4545-d88f-f439e42a83ab","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Before preprocessing\n","['A Beginner’s Guide to Word Embedding with Gensim Word2Vec\\xa0Model'\n"," 'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric'\n"," 'How to Use ggplot2 in\\xa0Python'\n"," 'Databricks: How to Save Files in CSV on Your Local\\xa0Computer'\n"," 'A Step-by-Step Implementation of Gradient Descent and Backpropagation']\n","After preprocessing\n","['A Beginners Guide to Word Embedding with Gensim Word2Vec Model', 'Handson Graph Neural Networks with PyTorch  PyTorch Geometric', 'How to Use ggplot2 in Python', 'Databricks How to Save Files in CSV on Your Local Computer', 'A StepbyStep Implementation of Gradient Descent and Backpropagation']\n"]}],"source":["def cleaning_text(text):\n","    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n","    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n","    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n","    return cleaned_text\n","\n","cleaned_data = list(map(cleaning_text, data)) # 모든 특수문자와 공백을 지움\n","print('Before preprocessing')\n","print(data[:5])\n","print('After preprocessing')\n","print(cleaned_data[:5])"]},{"cell_type":"markdown","metadata":{"id":"LnHz92EHD-M8"},"source":["#### 📝 설명: Tokenizer\n","* Tokenizer는 텍스트 데이터를 작은 단위로 분리해주는 도구입니다.\n","* 텍스트 데이터를 머신 러닝 모델에 입력으로 사용하거나 자연어 처리 작업을 수행할 때, 문장을 단어 또는 하위 단위(subword)로 분리하는 역할을 수행하기 위한 도구입니다.\n","  * 텍스트를 단어 또는 하위 단위로 분리 (토큰 분리): 텍스트를 띄어쓰기 단위로 나누거나, 보다 작은 단위로 분리합니다.\n","    * 예를 들어, \"I love PyTorch\"이라는 문장을 단어 단위로 분리하면 [\"I\", \"love\", \"PyTorch\"]과 같이 됩니다.\n","    * 하위 단위 토크나이저는 언어의 특성에 따라 단어를 더 작은 단위로 분리하여 처리할 수 있습니다. 예를 들어, \"playing\"이라는 단어를 \"play\"와 \"ing\"으로 분리하는 것입니다.\n","\n","  * 토큰을 숫자로 매핑: 머신 러닝 모델은 텍스트를 숫자로 처리해야 합니다. 따라서 모델이 텍스트를 처리할 수 있게 단어나 하위 단위를 고유한 숫자 ID로 매핑하는 작업을 수행합니다.\n","    * 예를 들어, [\"I\", \"love\", \"PyTorch\"] 이라는 단어들이 있을 때, 이를 이용하여 {\"I\":0, \"love\":1, \"PyTorch\":2}와 같은 단어 사전을 만들고, 이를 통해 [0, 1, 2]로 변환합니다.\n","  * 특수 토큰 추가: 텍스트를 모델에 입력으로 사용할 때, 특별한 의미를 가진 토큰을 추가할 수 있습니다.\n","    * 예를 들어 문장의 시작(<sos> 토큰)과 끝(<eox> 토큰)을 나타내는데 사용되거나, 미리 정의된 사전에 없는 단어를 대체하는데 사용될 수 있습니다.\n","    \n","* 자연어 처리를 위한 라이브러리인 `torchtext.vocab.build_vocab_from_iterator`를 이용하여 위 과정을 모두 쉽게 처리할 수 있습니다.\n","  \n","📚 참고할만한 자료:\n","* [torchtext getTokenizer](https://pytorch.org/text/stable/data_utils.html#get-tokenizer)\n","* [Vocab tokenize 설명](https://velog.io/@nkw011/nlp-vocab)"]},{"cell_type":"markdown","metadata":{"id":"BfvOgci0BIIy"},"source":["#### 📝 설명: build_vocab_from_iterator\n","`torchtext.vocab.build_vocab_from_iterator`는 iterator를 이용하여 Vocab 클래스(단어사전)를 만드는 함수입니다.\n","* 주요 parameter\n","  * iterator: 단어 사전을 만들 때 사용되는 iterator\n","  * min_freq: 단어 사전에 포함되기 위한 최소 빈도 수\n","* output\n","  * torchtext.vocab.Vocab 클래스를 반환합니다.\n","  * 이로써 Vocab class에 있는 함수들을 모두 사용할 수 있습니다.\n","\n","📚 참고할만한 자료:\n","* [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator)\n","* [Vocab class의 함수들](https://pytorch.org/text/stable/vocab.html)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9aCpr0QMD-M8","outputId":"517006d7-e55a-44b0-95db-4a7a875df86d","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Original text :  A Beginners Guide to Word Embedding with Gensim Word2Vec Model\n","Token:  ['a', 'beginners', 'guide', 'to', 'word', 'embedding', 'with', 'gensim', 'word2vec', 'model']\n"]}],"source":["# 토크나이저를 통해 단어 단위의 토큰을 생성합니다.\n","tokenizer = get_tokenizer(\"basic_english\")\n","tokens = tokenizer(cleaned_data[0])\n","print(\"Original text : \", cleaned_data[0])\n","print(\"Token: \", tokens)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"r7n43hkXD-M8","tags":[]},"outputs":[],"source":["# 단어 사전을 생성한 후, 시작과 끝 표시를 해줍니다.\n","vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, cleaned_data)) # 단어 사전을 생성합니다.\n","vocab.insert_token('<pad>', 0)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"PyhIkvA6D-M8","outputId":"0688b3e9-b1bd-4e8f-8eee-8728a406cc84","tags":[]},"outputs":[{"data":{"text/plain":["['<pad>', 'to', 'the', 'a', 'of', 'and', 'how', 'in', 'your', 'for']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["id2token = vocab.get_itos() # id to string\n","id2token[:10]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"s6tnF7m4D-M8","outputId":"22dd8636-14ec-4666-a909-0bdad4ffee4c","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<pad> 0\n","to 1\n","the 2\n","a 3\n","of 4\n","and 5\n"]}],"source":["token2id = vocab.get_stoi() # string to id\n","token2id = dict(sorted(token2id.items(), key=lambda item: item[1]))\n","for idx, (k,v) in enumerate(token2id.items()):\n","    print(k,v)\n","    if idx == 5:\n","        break"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"YJa-v2zaD-M_","outputId":"6e952f1b-2ee0-406e-c944-9cd628ab7abc","tags":[]},"outputs":[{"data":{"text/plain":["[3, 273, 66, 1, 467, 1582, 12, 2884, 8549, 99]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["vocab.lookup_indices(tokenizer(cleaned_data[0])) # 문장을 토큰화 후 id로 변환합니다."]},{"cell_type":"markdown","metadata":{"id":"IhFq1YSSD-M_"},"source":["#### 📝 설명: 데이터 전처리\n","\n","  \n","* input에 들어가는 단어 수가 모두 다르므로 이를 바로 모델에 넣기에는 어렵습니다. 이를 위해, \\<pad\\> (0)을 넣어서 길이를 맞춰주는 과정을 padding 이라고 합니다.\n","<!-- * label 값은 OneHotEncoding을 해야합니다.\n","  * torch.nn.functional.one_hot 함수를 이용하여 onehot encoding을 쉽게 할 수 있습니다.\n","  * OneHotEncoding이란? : 카테고리 형태의 데이터를 벡터로 변환하는 방법으로, 해당하는 카테고리에 해당하는 인덱스만 1이고 나머지는 모두 0인 이진 벡터로 표현하는 것을 의미합니다.\n","  * 왜 OneHotEncodingd을 해야할까? : multi-class(개, 고양이, 토끼 분류와 같은) 문제로 풀기 위함입니다.   -->\n","  \n","📚 참고할만한 자료:\n","* [Padding 설명](https://wikidocs.net/83544)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Klfeo2zvD-M_","tags":[]},"outputs":[],"source":["seq = []\n","for i in cleaned_data:\n","    token_id = vocab.lookup_indices(tokenizer(i))\n","    for j in range(1, len(token_id)):\n","        sequence = token_id[:j+1]\n","        seq.append(sequence)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"sB-FyKP_D-NA","outputId":"b89976f4-1c55-4600-c001-d02af1ed8039","tags":[]},"outputs":[{"data":{"text/plain":["[[3, 273],\n"," [3, 273, 66],\n"," [3, 273, 66, 1],\n"," [3, 273, 66, 1, 467],\n"," [3, 273, 66, 1, 467, 1582]]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["seq[:5]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["46380"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(seq)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"7BuVRGCmD-NA","outputId":"b236c1d8-36fd-47f6-e5d1-3563a2280de8","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["24\n"]}],"source":["max_len = max(len(sublist) for sublist in seq) # seq에 저장된 최대 토큰 길이 찾기\n","print(max_len)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"fdbAKwpDD-NA","outputId":"df9c7cc5-2e32-4059-b760-1decd7aec1a5","tags":[]},"outputs":[{"data":{"text/plain":["array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 273])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["def pre_zeropadding(seq, max_len): # max_len 길이에 맞춰서 0 으로 padding 처리 (앞부분에 padding 처리)\n","    return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n","zero_padding_data = pre_zeropadding(seq, max_len)\n","zero_padding_data[0] "]},{"cell_type":"code","execution_count":21,"metadata":{"id":"vMyauAQQD-NA","tags":[]},"outputs":[],"source":["input_x = zero_padding_data[:,:-1] # 마지막 값을 제외한 데이터 -> 입력\n","label = zero_padding_data[:,-1] # 마지막 값 데이터 -> 예측해야 되는 데이터 -> 출력"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"j5DfPs8MD-NA","outputId":"e15ca13e-7914-4c9c-b211-ef6566dae90d","tags":[]},"outputs":[{"data":{"text/plain":["array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   3],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   3, 273],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   3, 273,  66],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   3, 273,  66,   1],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   3, 273,  66,   1, 467]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["input_x[:5] # input 값 확인"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"hZleETe6D-NA","outputId":"46d55242-7b7a-40f3-af95-3dfae9c65a35","tags":[]},"outputs":[{"data":{"text/plain":["array([ 273,   66,    1,  467, 1582])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["label[:5] # label 값 확인"]},{"cell_type":"markdown","metadata":{"id":"ymKIegCwD-NA"},"source":["### 1-2 Custom Dataset 구현\n","\n","> 1-1에서 진행한 전처리 진행을 모듈화 시켜서 하나의 class로 구현합니다.\n"]},{"cell_type":"markdown","metadata":{"id":"ZyV5Yr34D-NB"},"source":["#### 📝 설명: Custom Dataset 정의하기\n","* 1-1에서 진행한 전처리 과정을 모두 함수화 시켜서 하나의 class로 구축합니다.\n","* 이로 인해, 손쉬운 모듈화가 가능합니다.\n","* 데이터를 변환하는 과정은 되도록이면 getitem 이 아닌 init 부분에 하여, 전처리하는 시간을 줄이도록 합니다.\n","  * init 부분에 한 번에 하게 되면 dataset을 정의할 때만 변환 시간이 소요되고, 그 이후로는 데이터 전처리 시간이 소요되지 않습니다.\n","\n","📚 참고할만한 자료:\n","* [Custom Dataset 구축 - Pytorch 공식 튜토리얼](https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"cmKLN7YXD-NF","tags":[]},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, vocab, tokenizer, max_len):\n","        self.data = data\n","        self.vocab = vocab\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        seq = self.make_sequence(self.data, self.vocab, self.tokenizer) # next word prediction을 하기 위한 형태로 변환\n","        self.seq = self.pre_zeropadding(seq, self.max_len) # zero padding으로 채워줌\n","        self.X = torch.tensor(self.seq[:,:-1]) # tensor 형태로 변환\n","        self.label = torch.tensor(self.seq[:,-1]) # tensor 형태로 변환\n","\n","    def make_sequence(self, data, vocab, tokenizer):\n","        seq = []\n","        for i in data:\n","            token_id = vocab.lookup_indices(tokenizer(i))\n","            for j in range(1, len(token_id)):\n","                sequence = token_id[:j+1]\n","                seq.append(sequence)\n","        return seq\n","\n","    def pre_zeropadding(self, seq, max_len): # max_len 길이에 맞춰서 0 으로 padding 처리 (앞부분에 padding 처리)\n","        return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n","\n","    def __len__(self): # dataset의 전체 길이 반환\n","        return len(self.X)\n","\n","    def __getitem__(self, idx): # dataset 접근\n","        X = self.X[idx]\n","        label = self.label[idx]\n","        return X, label"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"IyFqR5byD-NF","tags":[]},"outputs":[],"source":["def cleaning_text(text):\n","    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n","    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n","    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n","    return cleaned_text\n","\n","data = list(map(cleaning_text, data))\n","tokenizer = get_tokenizer(\"basic_english\")\n","vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, data))\n","vocab.insert_token('<pad>',0)\n","max_len = 20"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"_KqafQoED-NF","tags":[]},"outputs":[],"source":["# train set, validation set, test set으로 data set을 나눕니다. 8 : 1 : 1 의 비율로 나눕니다.\n","train, test = train_test_split(data, test_size = .2, random_state = 42)\n","val, test = train_test_split(test, test_size = .5, random_state = 42)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ZiBdur8sD-NF","outputId":"8fe72ec9-254a-4f73-ff86-47715c546c29","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train 개수:  5206\n","Validation 개수:  651\n","Test 개수:  651\n"]}],"source":["print(\"Train 개수: \", len(train))\n","print(\"Validation 개수: \", len(val))\n","print(\"Test 개수: \", len(test))"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Mg5ThrQnD-NG","tags":[]},"outputs":[],"source":["train_dataset = CustomDataset(train, vocab, tokenizer, max_len)\n","valid_dataset = CustomDataset(val, vocab, tokenizer, max_len)\n","test_dataset = CustomDataset(test, vocab, tokenizer, max_len)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"STAxoDp6D-NG","tags":[]},"outputs":[],"source":["batch_size = 32\n","\n","train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n","test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"]},{"cell_type":"markdown","metadata":{"id":"DAV7ClRZD-NG"},"source":["## 2. Next word prediction 모델 구현\n","\n","```\n","💡 목차 개요: Next word prediction을 위한 DNN 모델을 직접 구현하고, 이를 학습하여 봅니다.\n","```\n","\n","- 2-1. Next word prediction을 위한 DNN 모델 구현\n","- 2-2. 모델 학습 및 추론"]},{"cell_type":"markdown","metadata":{"id":"Ld9wg6caD-NG"},"source":["### 2-1 Next word prediction을 위한 DNN 모델 구축\n","\n","> Next word prediction을 위한 DNN 모델을 직접 구축해봅니다.\n"]},{"cell_type":"markdown","metadata":{"id":"gqBFtI8iD-NG"},"source":["#### 📝 설명: Next word prediction을 위한 DNN 모델 구축\n","* DNN 구현 (2)에서 학습하였던, DNN 모델을 기반에 `nn.Embedding`을 추가하여 next word prediction을 하기 위한 DNN 모델을 구축합니다.\n","* Embedding이란?\n","  * 텍스트나 범주형 데이터와 같이 모델이 처리하기 어려운 형태의 데이터를 수치 형태로 변환하는 기술입니다.\n","  * 주어진 데이터를 저차원의 벡터 공간에 표현하는 방법으로, 단어, 문장, 범주형 변수 등을 고정된 길이의 실수 벡터로 매핑하여 표현합니다.\n","* `nn.Embedding`\n","  * num_embedding : embedding할 input값의 수를 의미합니다. 자연어처리에선 단어 사전의 크기와 동일합니다.\n","  * embedding_dim : embedding 벡터의 차원을 의미합니다.\n","  \n","📚 참고할만한 자료:\n","* [torch.nn.Embedding - Pytorch 공식 튜토리얼](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","* [Embedding 설명](https://wikidocs.net/64779)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"so6MkCyYD-NG","tags":[]},"outputs":[],"source":["class NextWordPredictionModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dims, hidden_dims, num_classes, dropout_ratio, set_super):\n","        if set_super:\n","            super().__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dims, padding_idx = 0) # padding index 설정 => gradient 계산에서 제외\n","        self.hidden_dims = hidden_dims\n","        self.layers = nn.ModuleList()\n","        self.num_classes = num_classes\n","        for i in range(len(self.hidden_dims) - 1):\n","            self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))\n","\n","            self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1]))\n","\n","            self.layers.append(nn.ReLU())\n","\n","            self.layers.append(nn.Dropout(dropout_ratio))\n","\n","        self.classifier = nn.Linear(self.hidden_dims[-1], self.num_classes)\n","        self.softmax = nn.LogSoftmax(dim = 1)\n","\n","    def forward(self, x):\n","        '''\n","        INPUT:\n","            x: [batch_size, sequence_len] # padding 제외\n","        OUTPUT:\n","            output : [batch_size, vocab_size]\n","        '''\n","        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n","        x = torch.sum(x, dim=1) # [batch_size, embedding_dim] 각 문장에 대해 임베딩된 단어들을 합쳐서, 해당 문장에 대한 임베딩 벡터로 만들어줍니다.\n","        for layer in self.layers:\n","            x = layer(x)\n","\n","        output = self.classifier(x) # [batch_size, num_classes]\n","        output = self.softmax(output) # [batch_size, num_classes]\n","        return output\n","\n","    def count_parameters(self):\n","        return sum(p.numel() for p in self.parameters() if p.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"vV45NcTRD-NH"},"source":["### 2-2 모델 학습 및 추론\n","\n","> Next word prediction 모델을 직접 학습하고, text를 직접 넣어 next word prediction을 직접 수행해봅니다.\n"]},{"cell_type":"markdown","metadata":{"id":"eDt45BiTD-NH"},"source":["#### 📝 설명: Next word prediction 학습하기\n","* DNN 모델을 학습하기 위해 모델의 파라미터를 정해줍니다.\n","* embedding layer와 fully connected layer의 연산이 가능하게 하기 위해 hidden dimension 리스트 구성 시, embedding dimension을 첫번째 값으로 설정합니다.\n","* 예측하려는 label의 개수는 단어 사전에 있는 단어의 개수와 동일합니다."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"TxGR6EjKD-NH","tags":[]},"outputs":[],"source":["# training 코드, evaluation 코드, training loop 코드\n","def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n","    model.train()  # 모델을 학습 모드로 설정\n","    train_loss = 0.0\n","    train_accuracy = 0\n","\n","    tbar = tqdm(dataloader)\n","    for texts, labels in tbar:\n","        texts = texts.to(device)\n","        labels = labels.to(device)\n","\n","        # 순전파\n","        outputs = model(texts)\n","\n","        loss = criterion(outputs, labels)\n","\n","        # 역전파 및 가중치 업데이트\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # 손실과 정확도 계산\n","        train_loss += loss.item()\n","        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n","        _, predicted = torch.max(outputs, dim=1)\n","\n","\n","        train_accuracy += (predicted == labels).sum().item()\n","\n","        # tqdm의 진행바에 표시될 설명 텍스트를 설정\n","        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n","\n","    # 에폭별 학습 결과 출력\n","    train_loss = train_loss / len(dataloader)\n","    train_accuracy = train_accuracy / len(train_dataset)\n","\n","    return model, train_loss, train_accuracy\n","\n","def evaluation(model, dataloader, val_dataset, criterion, device, epoch, num_epochs):\n","    model.eval()  # 모델을 평가 모드로 설정\n","    valid_loss = 0.0\n","    valid_accuracy = 0\n","\n","    with torch.no_grad(): # model의 업데이트 막기\n","        tbar = tqdm(dataloader)\n","        for texts, labels in tbar:\n","            texts = texts.to(device)\n","            labels = labels.to(device)\n","\n","            # 순전파\n","            outputs = model(texts)\n","            loss = criterion(outputs, labels)\n","\n","            # 손실과 정확도 계산\n","            valid_loss += loss.item()\n","            # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n","            _, predicted = torch.max(outputs, 1)\n","            # _, true_labels = torch.max(labels, dim=1)\n","            valid_accuracy += (predicted == labels).sum().item()\n","\n","\n","            # tqdm의 진행바에 표시될 설명 텍스트를 설정\n","            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n","\n","    valid_loss = valid_loss / len(dataloader)\n","    valid_accuracy = valid_accuracy / len(val_dataset)\n","\n","    return model, valid_loss, valid_accuracy\n","\n","\n","def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n","    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n","    early_stop_counter = 0  # 카운터\n","    valid_max_accuracy = -1\n","\n","    for epoch in range(num_epochs):\n","        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n","        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n","\n","        if valid_accuracy > valid_max_accuracy:\n","            valid_max_accuracy = valid_accuracy\n","\n","        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n","            early_stop_counter = 0\n","\n","        # validation loss가 증가하거나 같으면 카운터 증가\n","        else:\n","            early_stop_counter += 1\n","\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n","\n","        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n","        if early_stop_counter >= patience:\n","            print(\"Early stopping\")\n","            break\n","\n","    return model, valid_max_accuracy"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"referenced_widgets":["71f0de6186ef4bcdbc72758e46fe8938","dceb9c596e5c496aa5859e86bc68af23","6ffa288ecd0044c8a09727c171f8f88d","1cd671b15ecd42ee991ebb02fedc0393","7cf1826bb9844825a53ef89121fa6a83","a31ea30e437d41b88fc90dbfd0259713","c5a607bcabc74ec99f648402fedaa615","712d7f6d2a3240b8a8b6743576d022cb","1462f4bbc7cb4d74a22a4a884419f6f1","68a08ddd157441fba42e017b30e604df","02ac2be4562b428499c70bc05d5030f3","3dd951f27a404f3dbfb1fd51afdac352","ab16ed0ddfdb4b51ab38c14f51c40c27","c9312430adbc4c15adbea6418e7cc758","5d8f08d1cf9c44fdbebd6d03551a7008","5e17a25efcc34e47af40696309570663","fc8d7429e50b4682923e8c5991ba3a4e","eb426cd6107a4ab3844c7d2e36b08320"]},"id":"fSdBdm4JD-NH","outputId":"a2c30596-07b0-4da1-fba5-88b7d150c05b","tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62dd65c938e44f6fa03419bead9df267","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bde2b8a46ebb451f95f8c38a67127aef","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Train Loss: 7.3743, Train Accuracy: 0.0633 Valid Loss: 7.2358, Valid Accuracy: 0.0671\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85495051c6bc48d28ea91cebb3950725","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d8a23d753bd4ce081482edc3a845b7e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [2/100], Train Loss: 6.7342, Train Accuracy: 0.0759 Valid Loss: 7.2108, Valid Accuracy: 0.0779\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66b7809542984094beb5582e91d346ce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea29e9f93b524c899cb79a2e57ed7c0a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [3/100], Train Loss: 6.3774, Train Accuracy: 0.0843 Valid Loss: 7.3121, Valid Accuracy: 0.0813\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc7e98a23bf04007a441f2f80d16c48c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcb67fd309b34e0abce6b14ac0ad06e2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [4/100], Train Loss: 6.0696, Train Accuracy: 0.0944 Valid Loss: 7.4566, Valid Accuracy: 0.0880\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d766604000c4b86ba5774405a4e8ad5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1efbd6d95fd54d6490728fc4db52cc77","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [5/100], Train Loss: 5.8033, Train Accuracy: 0.1034 Valid Loss: 7.6273, Valid Accuracy: 0.0935\n","Early stopping\n","Valid max accuracy :  0.09349936682144365\n"]}],"source":["lr = 1e-3\n","vocab_size = len(vocab.get_stoi())\n","embedding_dims = 512\n","hidden_dims = [embedding_dims, embedding_dims*4, embedding_dims*2, embedding_dims]\n","model = NextWordPredictionModel(vocab_size = vocab_size, embedding_dims = embedding_dims, hidden_dims = hidden_dims, num_classes = vocab_size, \\\n","            dropout_ratio = 0.2, set_super = True).to(device)\n","\n","num_epochs = 100\n","patience = 3\n","model_name = 'next'\n","\n","optimizer = optim.Adam(model.parameters(), lr = lr)\n","criterion = nn.NLLLoss(ignore_index=0) # padding 한 부분 제외\n","model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n","print('Valid max accuracy : ', valid_max_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"70lYJ9aED-NH"},"source":["#### 📝 설명: Next word prediction 평가하기\n","* 학습한 DNN 모델을 accuracy score로 평가합니다."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"referenced_widgets":["76532b6300d844368e015b4f95d4561d","53224ea7733a497cbcb5da9be2817102"]},"id":"jK4sD9h2D-NH","outputId":"e8bd5426-2cd8-4059-80a6-797bd834791c","tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28b1f49da9cb4d63ab3f495c60b2471f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Next word prediction DNN model accuracy :  0.07480314960629922\n"]}],"source":["model.load_state_dict(torch.load(\"./model_next.pt\")) # 모델 불러오기\n","model = model.to(device)\n","model.eval()\n","total_labels = []\n","total_preds = []\n","with torch.no_grad():\n","    for texts, labels in tqdm(test_dataloader):\n","        texts = texts.to(device)\n","        labels = labels\n","\n","        outputs = model(texts)\n","        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        total_preds.extend(predicted.detach().cpu().tolist())\n","        total_labels.extend(labels.tolist())\n","\n","total_preds = np.array(total_preds)\n","total_labels = np.array(total_labels)\n","nwp_dnn_acc = accuracy_score(total_labels, total_preds) # 정확도 계산\n","print(\"Next word prediction DNN model accuracy : \", nwp_dnn_acc)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1692592994519,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"OFsMBAeBZ1q0","outputId":"d2edb1d4-2a3d-406e-88d5-b12c71d13c42"},"outputs":[{"name":"stdout","output_type":"stream","text":["8618\n"]}],"source":["print(vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"c1HHIbXrp5pZ"},"source":["## Required Package\n","\n","> torch == 2.0.1\n","\n","> torchvision == 0.15.2\n","\n","> sklearn == 1.3.0\n","\n","> torchtext == 0.15.2"]},{"cell_type":"markdown","metadata":{"id":"k6fnxdyLp2Qk"},"source":["## 콘텐츠 라이선스\n","\n","저작권 : <font color='blue'> <b> ©2023 by Upstage X fastcampus Co., Ltd. All rights reserved.</font></b>\n","\n","<font color='red'><b>WARNING</font> : 본 교육 콘텐츠의 지식재산권은 업스테이지 및 패스트캠퍼스에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다. </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWBuGG4vhz1W"},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
