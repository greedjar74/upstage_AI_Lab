{"cells":[{"cell_type":"markdown","metadata":{"id":"biI2UMXHon5o"},"source":["# DNN êµ¬í˜„(3)"]},{"cell_type":"markdown","metadata":{"id":"zOtbGaE_ojQA"},"source":["## ì‹¤ìŠµ ê°œìš”\n","\n","1) **ì‹¤ìŠµ ëª©ì **\n","\n","ì´ë²ˆ ì‹¤ìŠµì€ ì´ë¡ ìœ¼ë¡œ ë°°ì› ë˜ PyTorchë¥¼ ì´ìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ ë°ì´í„°ì…‹ì„ ì§ì ‘ êµ¬ì¶•í•˜ê³ , ì´ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ë‹ˆë‹¤.\n","\n","\n","2) **ìˆ˜ê°• ëª©í‘œ**\n","\n","- PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ Custom Datasetì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.\n","- ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë¸ì„ êµ¬ì¶•í•  ë•Œì˜ ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤.\n","- DNNìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤."]},{"cell_type":"markdown","metadata":{"id":"vUnGWX59ybEk"},"source":["### ì‹¤ìŠµ ëª©ì°¨\n","* 1. Custom Dataset êµ¬ì¶•í•˜ê¸°\n","  * 1-1. ìì—°ì–´ ë°ì´í„°ì˜ ì „ì²˜ë¦¬\n","  * 1-2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶•í•˜ê¸°\n","\n","* 2. Next word prediction ëª¨ë¸ êµ¬ì¶•\n","  * 2-1. Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬ì¶•\n","  * 2-2. ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ \n"]},{"cell_type":"markdown","metadata":{"id":"3F2fMLWA31Ft"},"source":["### í™˜ê²½ ì„¤ì •\n","\n","- íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì„í¬íŠ¸"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24494,"status":"ok","timestamp":1692592348488,"user":{"displayName":"Eddie(ê¹€ìœ¤ê¸°)","userId":"11850705511374304262"},"user_tz":-540},"id":"_wvClBdFmDml","outputId":"46393e01-d12d-481c-f661-8ed70cf48cad"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.11/site-packages/PyBioMed-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# !pip install scikit-learn==1.3.0 -q\n","# !pip install torch==2.0.1 -q\n","# !pip install torchvision==0.15.2 -q\n","!pip install torchtext==0.15.2 -q"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GKbkjkhbiPNt","tags":[]},"outputs":[],"source":["import numpy as np # ê¸°ë³¸ì ì¸ ì—°ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import matplotlib.pyplot as plt # ê·¸ë¦¼ì´ë‚˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","from tqdm.notebook import tqdm # ìƒíƒœ ë°”ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import pandas as pd # ë°ì´í„°í”„ë ˆì„ì„ ì¡°ì‘í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","\n","import torch # PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import torch.nn as nn # ëª¨ë¸ êµ¬ì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import torch.optim as optim # optimizer ì„¤ì •ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","from torch.utils.data import Dataset, DataLoader # ë°ì´í„°ì…‹ ì„¤ì •ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","\n","from torchtext.data import get_tokenizer # torchì—ì„œ tokenizerë¥¼ ì–»ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import torchtext # torchì—ì„œ textë¥¼ ë” ì˜ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","\n","from sklearn.metrics import accuracy_score # ì„±ëŠ¥ì§€í‘œ ì¸¡ì •\n","from sklearn.model_selection import train_test_split # train-validation-test set ë‚˜ëˆ„ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n","\n","import re # text ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qnIkzdqds7t4","tags":[]},"outputs":[],"source":["# seed ê³ ì •\n","import random\n","import torch.backends.cudnn as cudnn\n","\n","def random_seed(seed_num):\n","    torch.manual_seed(seed_num)\n","    torch.cuda.manual_seed(seed_num)\n","    torch.cuda.manual_seed_all(seed_num)\n","    np.random.seed(seed_num)\n","    cudnn.benchmark = False\n","    cudnn.deterministic = True\n","    random.seed(seed_num)\n","\n","random_seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Bu6Ag-QeD-M5","tags":[]},"outputs":[],"source":["device = 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"expqZQX60qa6"},"source":["###  ë°ì´í„° ì…‹ ê°œìš” </b>\n","\n","* ë°ì´í„°ì…‹: <a href='https://www.kaggle.com/datasets/dorianlazar/medium-articles-dataset'>Medium Dataset</a>\n","* ë°ì´í„°ì…‹ ê°œìš”: \"Towards Data Science\", \"UX Collective\", \"The Startup\", \"The Writing Cooperative\", \"Data Driven Investor\", \"Better Humans\", \"Better Marketing\" ì˜ 7ê°œì˜ ì£¼ì œë¥¼ ê°€ì§€ëŠ” publication ì— ëŒ€í•´ì„œ í¬ë¡¤ë§ì„ í•œ ë°ì´í„°ì…ë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ëŠ” ì´ 6,508ê°œì˜ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ì™€ ë©”íƒ€ ë°ì´í„°(.csv)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì‹¤ìŠµì—ì„œëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ CustomDatasetì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n","  * [How to collect ths dataset?](https://dorianlazar.medium.com/scraping-medium-with-python-beautiful-soup-3314f898bbf5)\n","- ë©”íƒ€ ë°ì´í„° ìŠ¤í‚¤ë§ˆ: ë©”íƒ€ ë°ì´í„°ëŠ” ì´ **10**ê°œì˜ columnìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n","  - id: ì•„ì´ë””\n","  - url: í¬ìŠ¤íŒ… ë§í¬\n","  - title: ì œëª©\n","  - subtitle: ë¶€ì œëª©\n","  - image: í¬ìŠ¤íŒ… ì´ë¯¸ì§€ì˜ íŒŒì¼ ì´ë¦„\n","  - claps: ì¶”ì²œ ìˆ˜\n","  - reponses: ëŒ“ê¸€ ìˆ˜\n","  - reading_time: ì½ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„\n","  - publication: ì£¼ì œ ì¹´í…Œê³ ë¦¬(e.g. Towards Data Science..)\n","  - date: ì‘ì„± ë‚ ì§œ\n","- ë°ì´í„° ì…‹ ì €ì‘ê¶Œ: CC0: Public Domain"]},{"cell_type":"markdown","metadata":{"id":"ga4KpW8DED_Q"},"source":["## 1. Custom Dataset êµ¬ì¶•í•˜ê¸°\n","\n","```\n","ğŸ’¡ ëª©ì°¨ ê°œìš”: ìì—°ì–´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ custom dataset class ë¥¼ ì§ì ‘ êµ¬í˜„í•´ë´…ë‹ˆë‹¤.\n","```\n","\n","- 1-1. ìì—°ì–´ ë°ì´í„°ì˜ ì „ì²˜ë¦¬\n","- 1-2. Custom Dataset class êµ¬ì¶•í•˜ê¸°\n"]},{"cell_type":"markdown","metadata":{"id":"7a3AnRDwEu4u"},"source":["### 1-1 ìì—°ì–´ ë°ì´í„° ì „ì²˜ë¦¬\n","\n","> textë¡œ ëœ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ìˆ«ì í˜•ì‹ìœ¼ë¡œ ë°”ê¾¸ê³ , ëª¨ë¸ì— ë„£ëŠ” êµ¬ì¡°ë¡œ ë°”ê¾¸ëŠ”ì§€ ì§ì ‘ ì‹¤ìŠµí•´ë´…ë‹ˆë‹¤.\n"]},{"cell_type":"markdown","metadata":{"id":"cu08GdrID-M6"},"source":["#### ğŸ“ ì„¤ëª…: Next word prediction\n","* ê¸€ì˜ ì¼ë¶€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡ (next word prediction)í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n","* ì˜ˆë¥¼ ë“¤ì–´, \"ë‚˜ëŠ” í•™êµë¥¼ ê°€ì„œ ë°¥ì„ ë¨¹ì—ˆë‹¤.\" ë¼ëŠ” ë¬¸ì¥ì´ ì£¼ì–´ì§„ë‹¤ê³  í•´ë´…ì‹œë‹¤.\n","\n","|input|label|\n","|------|---|\n","|ë‚˜ëŠ”|í•™êµë¥¼|\n","|ë‚˜ëŠ” í•™êµë¥¼|ê°€ì„œ|\n","|ë‚˜ëŠ” í•™êµë¥¼ ê°€ì„œ|ë°¥ì„|\n","|ë‚˜ëŠ” í•™êµë¥¼ ê°€ì„œ ë°¥ì„|ë¨¹ì—ˆë‹¤.|\n","\n","* ì´ì™€ ê°™ì´ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³ , DNNì„ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ë´…ë‹ˆë‹¤.\n","\n","ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:\n","* [Next word prediction](https://wikidocs.net/45101)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"elapsed":303,"status":"ok","timestamp":1692592944318,"user":{"displayName":"Eddie(ê¹€ìœ¤ê¸°)","userId":"11850705511374304262"},"user_tz":-540},"id":"CZ-udSLHrqSl","outputId":"5c39f601-68dc-483a-c0c5-2d452acc3824","tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url</th>\n","      <th>title</th>\n","      <th>subtitle</th>\n","      <th>image</th>\n","      <th>claps</th>\n","      <th>responses</th>\n","      <th>reading_time</th>\n","      <th>publication</th>\n","      <th>date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n","      <td>A Beginnerâ€™s Guide to Word Embedding with Gens...</td>\n","      <td>NaN</td>\n","      <td>1.png</td>\n","      <td>850</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n","      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n","      <td>NaN</td>\n","      <td>2.png</td>\n","      <td>1100</td>\n","      <td>11</td>\n","      <td>9</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n","      <td>How to Use ggplot2 inÂ Python</td>\n","      <td>A Grammar of Graphics forÂ Python</td>\n","      <td>3.png</td>\n","      <td>767</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>https://towardsdatascience.com/databricks-how-...</td>\n","      <td>Databricks: How to Save Files in CSV on Your L...</td>\n","      <td>When I work on Python projectsÂ dealingâ€¦</td>\n","      <td>4.jpeg</td>\n","      <td>354</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n","      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n","      <td>One example of buildingÂ neuralâ€¦</td>\n","      <td>5.jpeg</td>\n","      <td>211</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>Towards Data Science</td>\n","      <td>2019-05-30</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                                url  \\\n","0   1  https://towardsdatascience.com/a-beginners-gui...   \n","1   2  https://towardsdatascience.com/hands-on-graph-...   \n","2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n","3   4  https://towardsdatascience.com/databricks-how-...   \n","4   5  https://towardsdatascience.com/a-step-by-step-...   \n","\n","                                               title  \\\n","0  A Beginnerâ€™s Guide to Word Embedding with Gens...   \n","1  Hands-on Graph Neural Networks with PyTorch & ...   \n","2                       How to Use ggplot2 inÂ Python   \n","3  Databricks: How to Save Files in CSV on Your L...   \n","4  A Step-by-Step Implementation of Gradient Desc...   \n","\n","                                  subtitle   image  claps responses  \\\n","0                                      NaN   1.png    850         8   \n","1                                      NaN   2.png   1100        11   \n","2         A Grammar of Graphics forÂ Python   3.png    767         1   \n","3  When I work on Python projectsÂ dealingâ€¦  4.jpeg    354         0   \n","4          One example of buildingÂ neuralâ€¦  5.jpeg    211         3   \n","\n","   reading_time           publication        date  \n","0             8  Towards Data Science  2019-05-30  \n","1             9  Towards Data Science  2019-05-30  \n","2             5  Towards Data Science  2019-05-30  \n","3             4  Towards Data Science  2019-05-30  \n","4             4  Towards Data Science  2019-05-30  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data_csv = pd.read_csv('./data/medium_data.csv')\n","data_csv.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GiuA9zb9rBkc","outputId":"21aa133b-50f6-4e0e-ddbe-630a0a5d1959","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(6508, 10)\n"]}],"source":["print(data_csv.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vzRs-fN_sMB3","tags":[]},"outputs":[],"source":["# ê°ê°ì˜ titleë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","# ìš°ë¦¬ëŠ” titleì˜ ì²« ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤.\n","data = data_csv['title'].values"]},{"cell_type":"markdown","metadata":{"id":"JF2DjmcXD-M7"},"source":["#### ğŸ“ ì„¤ëª…: í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n","* í•´ë‹¹ ë°ì´í„°ì…‹ì€ í¬ë¡¤ë§(ì¸í„°ë„·ì— ìˆëŠ” ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê¸°ë²•)ì„ í†µí•´ êµ¬ì¶•ë˜ì—ˆê¸° ë•Œë¬¸ì— no-break spaceê°€ ì¢…ì¢… ë°œìƒí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ no-break spaceë¥¼ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n","  * No-Break Spaceë€? ì›¹ í˜ì´ì§€ë‚˜ ë¬¸ì„œ ë“±ì—ì„œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ ì‚¬ì´ì˜ ê³µë°±ì´ ìˆëŠ” ê²½ìš°, í•´ë‹¹ ê³µë°±ì´ ì¤„ ë°”ê¿ˆìœ¼ë¡œ ì¸í•´ ë¶„ë¦¬ë˜ì§€ ì•Šê³  í•œ ë‹¨ì–´ë‚˜ ë¬¸ì¥ìœ¼ë¡œ ì¸ì‹ë˜ë„ë¡ í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ê³µë°±\n","  * ì˜ˆì‹œ (no-break-space ì‚¬ìš© X)\n","    ```\n","    Hello\n","    World~\n","    ```\n","    \n","    (no-break-space ì‚¬ìš©)\n","    ```\n","    Hello,âµworld!\n","    ```\n","* no-break spaceë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ì„  unicode í˜•ì‹ìœ¼ë¡œ ì œê±°ë¥¼ í•´ì•¼í•©ë‹ˆë‹¤.\n","  * unicodeë€? ì „ ì„¸ê³„ì˜ ëª¨ë“  ë¬¸ìì™€ ê¸°í˜¸ë¥¼ ì¼ê´€ì„± ìˆê²Œ í‘œí˜„í•˜ê¸° ìœ„í•œ í‘œì¤€ ë¬¸ì ì¸ì½”ë”© ì²´ê³„\n","\n","* `re` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:\n","* <a href='https://www.compart.com/en/unicode'>unicode ê²€ìƒ‰ ì‚¬ì´íŠ¸</a>\n","* [re ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©ë²•](https://velog.io/@hoegon02/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-12-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%A0%84%EC%B2%98%EB%A6%AC-%EC%A0%95%EA%B7%9C-%ED%91%9C%ED%98%84%EC%8B%9D-3qmtwryf)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"kFmwB6NWD-M7","outputId":"76492532-2031-4545-d88f-f439e42a83ab","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Before preprocessing\n","['A Beginnerâ€™s Guide to Word Embedding with Gensim Word2Vec\\xa0Model'\n"," 'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric'\n"," 'How to Use ggplot2 in\\xa0Python'\n"," 'Databricks: How to Save Files in CSV on Your Local\\xa0Computer'\n"," 'A Step-by-Step Implementation of Gradient Descent and Backpropagation']\n","After preprocessing\n","['A Beginners Guide to Word Embedding with Gensim Word2Vec Model', 'Handson Graph Neural Networks with PyTorch  PyTorch Geometric', 'How to Use ggplot2 in Python', 'Databricks How to Save Files in CSV on Your Local Computer', 'A StepbyStep Implementation of Gradient Descent and Backpropagation']\n"]}],"source":["def cleaning_text(text):\n","    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # íŠ¹ìˆ˜ë¬¸ì ë¥¼ ëª¨ë‘ ì§€ìš°ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n","    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break spaceë¥¼ unicode ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜\n","    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode ë¹ˆì¹¸ì„ ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜\n","    return cleaned_text\n","\n","cleaned_data = list(map(cleaning_text, data)) # ëª¨ë“  íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°±ì„ ì§€ì›€\n","print('Before preprocessing')\n","print(data[:5])\n","print('After preprocessing')\n","print(cleaned_data[:5])"]},{"cell_type":"markdown","metadata":{"id":"LnHz92EHD-M8"},"source":["#### ğŸ“ ì„¤ëª…: Tokenizer\n","* TokenizerëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n","* í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ, ë¬¸ì¥ì„ ë‹¨ì–´ ë˜ëŠ” í•˜ìœ„ ë‹¨ìœ„(subword)ë¡œ ë¶„ë¦¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.\n","  * í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë˜ëŠ” í•˜ìœ„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í† í° ë¶„ë¦¬): í…ìŠ¤íŠ¸ë¥¼ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê±°ë‚˜, ë³´ë‹¤ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n","    * ì˜ˆë¥¼ ë“¤ì–´, \"I love PyTorch\"ì´ë¼ëŠ” ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ë©´ [\"I\", \"love\", \"PyTorch\"]ê³¼ ê°™ì´ ë©ë‹ˆë‹¤.\n","    * í•˜ìœ„ ë‹¨ìœ„ í† í¬ë‚˜ì´ì €ëŠ” ì–¸ì–´ì˜ íŠ¹ì„±ì— ë”°ë¼ ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, \"playing\"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ \"play\"ì™€ \"ing\"ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n","\n","  * í† í°ì„ ìˆ«ìë¡œ ë§¤í•‘: ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë‹¨ì–´ë‚˜ í•˜ìœ„ ë‹¨ìœ„ë¥¼ ê³ ìœ í•œ ìˆ«ì IDë¡œ ë§¤í•‘í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n","    * ì˜ˆë¥¼ ë“¤ì–´, [\"I\", \"love\", \"PyTorch\"] ì´ë¼ëŠ” ë‹¨ì–´ë“¤ì´ ìˆì„ ë•Œ, ì´ë¥¼ ì´ìš©í•˜ì—¬ {\"I\":0, \"love\":1, \"PyTorch\":2}ì™€ ê°™ì€ ë‹¨ì–´ ì‚¬ì „ì„ ë§Œë“¤ê³ , ì´ë¥¼ í†µí•´ [0, 1, 2]ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n","  * íŠ¹ìˆ˜ í† í° ì¶”ê°€: í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ë•Œ, íŠ¹ë³„í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ í† í°ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","    * ì˜ˆë¥¼ ë“¤ì–´ ë¬¸ì¥ì˜ ì‹œì‘(<sos> í† í°)ê³¼ ë(<eox> í† í°)ì„ ë‚˜íƒ€ë‚´ëŠ”ë° ì‚¬ìš©ë˜ê±°ë‚˜, ë¯¸ë¦¬ ì •ì˜ëœ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ëŒ€ì²´í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","    \n","* ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ `torchtext.vocab.build_vocab_from_iterator`ë¥¼ ì´ìš©í•˜ì—¬ ìœ„ ê³¼ì •ì„ ëª¨ë‘ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","  \n","ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:\n","* [torchtext getTokenizer](https://pytorch.org/text/stable/data_utils.html#get-tokenizer)\n","* [Vocab tokenize ì„¤ëª…](https://velog.io/@nkw011/nlp-vocab)"]},{"cell_type":"markdown","metadata":{"id":"BfvOgci0BIIy"},"source":["#### ğŸ“ ì„¤ëª…: build_vocab_from_iterator\n","`torchtext.vocab.build_vocab_from_iterator`ëŠ” iteratorë¥¼ ì´ìš©í•˜ì—¬ Vocab í´ë˜ìŠ¤(ë‹¨ì–´ì‚¬ì „)ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n","* ì£¼ìš” parameter\n","  * iterator: ë‹¨ì–´ ì‚¬ì „ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©ë˜ëŠ” iterator\n","  * min_freq: ë‹¨ì–´ ì‚¬ì „ì— í¬í•¨ë˜ê¸° ìœ„í•œ ìµœì†Œ ë¹ˆë„ ìˆ˜\n","* output\n","  * torchtext.vocab.Vocab í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n","  * ì´ë¡œì¨ Vocab classì— ìˆëŠ” í•¨ìˆ˜ë“¤ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:\n","* [build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator)\n","* [Vocab classì˜ í•¨ìˆ˜ë“¤](https://pytorch.org/text/stable/vocab.html)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9aCpr0QMD-M8","outputId":"517006d7-e55a-44b0-95db-4a7a875df86d","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Original text :  A Beginners Guide to Word Embedding with Gensim Word2Vec Model\n","Token:  ['a', 'beginners', 'guide', 'to', 'word', 'embedding', 'with', 'gensim', 'word2vec', 'model']\n"]}],"source":["# í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ë‹¨ì–´ ë‹¨ìœ„ì˜ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.\n","tokenizer = get_tokenizer(\"basic_english\")\n","tokens = tokenizer(cleaned_data[0])\n","print(\"Original text : \", cleaned_data[0])\n","print(\"Token: \", tokens)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"r7n43hkXD-M8","tags":[]},"outputs":[],"source":["# ë‹¨ì–´ ì‚¬ì „ì„ ìƒì„±í•œ í›„, ì‹œì‘ê³¼ ë í‘œì‹œë¥¼ í•´ì¤ë‹ˆë‹¤.\n","vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, cleaned_data)) # ë‹¨ì–´ ì‚¬ì „ì„ ìƒì„±í•©ë‹ˆë‹¤.\n","vocab.insert_token('<pad>', 0)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"PyhIkvA6D-M8","outputId":"0688b3e9-b1bd-4e8f-8eee-8728a406cc84","tags":[]},"outputs":[{"data":{"text/plain":["['<pad>', 'to', 'the', 'a', 'of', 'and', 'how', 'in', 'your', 'for']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["id2token = vocab.get_itos() # id to string\n","id2token[:10]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"s6tnF7m4D-M8","outputId":"22dd8636-14ec-4666-a909-0bdad4ffee4c","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<pad> 0\n","to 1\n","the 2\n","a 3\n","of 4\n","and 5\n"]}],"source":["token2id = vocab.get_stoi() # string to id\n","token2id = dict(sorted(token2id.items(), key=lambda item: item[1]))\n","for idx, (k,v) in enumerate(token2id.items()):\n","    print(k,v)\n","    if idx == 5:\n","        break"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"YJa-v2zaD-M_","outputId":"6e952f1b-2ee0-406e-c944-9cd628ab7abc","tags":[]},"outputs":[{"data":{"text/plain":["[3, 273, 66, 1, 467, 1582, 12, 2884, 8549, 99]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["vocab.lookup_indices(tokenizer(cleaned_data[0])) # ë¬¸ì¥ì„ í† í°í™” í›„ idë¡œ ë³€í™˜í•©ë‹ˆë‹¤."]},{"cell_type":"markdown","metadata":{"id":"IhFq1YSSD-M_"},"source":["#### ğŸ“ ì„¤ëª…: ë°ì´í„° ì „ì²˜ë¦¬\n","\n","  \n","* inputì— ë“¤ì–´ê°€ëŠ” ë‹¨ì–´ ìˆ˜ê°€ ëª¨ë‘ ë‹¤ë¥´ë¯€ë¡œ ì´ë¥¼ ë°”ë¡œ ëª¨ë¸ì— ë„£ê¸°ì—ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, \\<pad\\> (0)ì„ ë„£ì–´ì„œ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ëŠ” ê³¼ì •ì„ padding ì´ë¼ê³  í•©ë‹ˆë‹¤.\n","<!-- * label ê°’ì€ OneHotEncodingì„ í•´ì•¼í•©ë‹ˆë‹¤.\n","  * torch.nn.functional.one_hot í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ onehot encodingì„ ì‰½ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","  * OneHotEncodingì´ë€? : ì¹´í…Œê³ ë¦¬ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ì¸ ì´ì§„ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n","  * ì™œ OneHotEncodingdì„ í•´ì•¼í• ê¹Œ? : multi-class(ê°œ, ê³ ì–‘ì´, í† ë¼ ë¶„ë¥˜ì™€ ê°™ì€) ë¬¸ì œë¡œ í’€ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.   -->\n","  \n","ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:\n","* [Padding ì„¤ëª…](https://wikidocs.net/83544)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Klfeo2zvD-M_","tags":[]},"outputs":[],"source":["seq = []\n","for i in cleaned_data:\n","    token_id = vocab.lookup_indices(tokenizer(i))\n","    for j in range(1, len(token_id)):\n","        sequence = token_id[:j+1]\n","        seq.append(sequence)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"sB-FyKP_D-NA","outputId":"b89976f4-1c55-4600-c001-d02af1ed8039","tags":[]},"outputs":[{"data":{"text/plain":["[[3, 273],\n"," [3, 273, 66],\n"," [3, 273, 66, 1],\n"," [3, 273, 66, 1, 467],\n"," [3, 273, 66, 1, 467, 1582]]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["seq[:5]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["46380"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(seq)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"7BuVRGCmD-NA","outputId":"b236c1d8-36fd-47f6-e5d1-3563a2280de8","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["24\n"]}],"source":["max_len = max(len(sublist) for sublist in seq) # seqì— ì €ì¥ëœ ìµœëŒ€ í† í° ê¸¸ì´ ì°¾ê¸°\n","print(max_len)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"fdbAKwpDD-NA","outputId":"df9c7cc5-2e32-4059-b760-1decd7aec1a5","tags":[]},"outputs":[{"data":{"text/plain":["array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 273])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["def pre_zeropadding(seq, max_len): # max_len ê¸¸ì´ì— ë§ì¶°ì„œ 0 ìœ¼ë¡œ padding ì²˜ë¦¬ (ì•ë¶€ë¶„ì— padding ì²˜ë¦¬)\n","    return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n","zero_padding_data = pre_zeropadding(seq, max_len)\n","zero_padding_data[0] "]},{"cell_type":"code","execution_count":21,"metadata":{"id":"vMyauAQQD-NA","tags":[]},"outputs":[],"source":["input_x = zero_padding_data[:,:-1] # ë§ˆì§€ë§‰ ê°’ì„ ì œì™¸í•œ ë°ì´í„° -> ì…ë ¥\n","label = zero_padding_data[:,-1] # ë§ˆì§€ë§‰ ê°’ ë°ì´í„° -> ì˜ˆì¸¡í•´ì•¼ ë˜ëŠ” ë°ì´í„° -> ì¶œë ¥"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"j5DfPs8MD-NA","outputId":"e15ca13e-7914-4c9c-b211-ef6566dae90d","tags":[]},"outputs":[{"data":{"text/plain":["array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   3],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   3, 273],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   3, 273,  66],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   3, 273,  66,   1],\n","       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   3, 273,  66,   1, 467]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["input_x[:5] # input ê°’ í™•ì¸"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"hZleETe6D-NA","outputId":"46d55242-7b7a-40f3-af95-3dfae9c65a35","tags":[]},"outputs":[{"data":{"text/plain":["array([ 273,   66,    1,  467, 1582])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["label[:5] # label ê°’ í™•ì¸"]},{"cell_type":"markdown","metadata":{"id":"ymKIegCwD-NA"},"source":["### 1-2 Custom Dataset êµ¬í˜„\n","\n","> 1-1ì—ì„œ ì§„í–‰í•œ ì „ì²˜ë¦¬ ì§„í–‰ì„ ëª¨ë“ˆí™” ì‹œì¼œì„œ í•˜ë‚˜ì˜ classë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.\n"]},{"cell_type":"markdown","metadata":{"id":"ZyV5Yr34D-NB"},"source":["#### ğŸ“ ì„¤ëª…: Custom Dataset ì •ì˜í•˜ê¸°\n","* 1-1ì—ì„œ ì§„í–‰í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ëª¨ë‘ í•¨ìˆ˜í™” ì‹œì¼œì„œ í•˜ë‚˜ì˜ classë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.\n","* ì´ë¡œ ì¸í•´, ì†ì‰¬ìš´ ëª¨ë“ˆí™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n","* ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ê³¼ì •ì€ ë˜ë„ë¡ì´ë©´ getitem ì´ ì•„ë‹Œ init ë¶€ë¶„ì— í•˜ì—¬, ì „ì²˜ë¦¬í•˜ëŠ” ì‹œê°„ì„ ì¤„ì´ë„ë¡ í•©ë‹ˆë‹¤.\n","  * init ë¶€ë¶„ì— í•œ ë²ˆì— í•˜ê²Œ ë˜ë©´ datasetì„ ì •ì˜í•  ë•Œë§Œ ë³€í™˜ ì‹œê°„ì´ ì†Œìš”ë˜ê³ , ê·¸ ì´í›„ë¡œëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ì‹œê°„ì´ ì†Œìš”ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n","\n","ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:\n","* [Custom Dataset êµ¬ì¶• - Pytorch ê³µì‹ íŠœí† ë¦¬ì–¼](https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"cmKLN7YXD-NF","tags":[]},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, vocab, tokenizer, max_len):\n","        self.data = data\n","        self.vocab = vocab\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        seq = self.make_sequence(self.data, self.vocab, self.tokenizer) # next word predictionì„ í•˜ê¸° ìœ„í•œ í˜•íƒœë¡œ ë³€í™˜\n","        self.seq = self.pre_zeropadding(seq, self.max_len) # zero paddingìœ¼ë¡œ ì±„ì›Œì¤Œ\n","        self.X = torch.tensor(self.seq[:,:-1]) # tensor í˜•íƒœë¡œ ë³€í™˜\n","        self.label = torch.tensor(self.seq[:,-1]) # tensor í˜•íƒœë¡œ ë³€í™˜\n","\n","    def make_sequence(self, data, vocab, tokenizer):\n","        seq = []\n","        for i in data:\n","            token_id = vocab.lookup_indices(tokenizer(i))\n","            for j in range(1, len(token_id)):\n","                sequence = token_id[:j+1]\n","                seq.append(sequence)\n","        return seq\n","\n","    def pre_zeropadding(self, seq, max_len): # max_len ê¸¸ì´ì— ë§ì¶°ì„œ 0 ìœ¼ë¡œ padding ì²˜ë¦¬ (ì•ë¶€ë¶„ì— padding ì²˜ë¦¬)\n","        return np.array([i[:max_len] if len(i) >= max_len else [0] * (max_len - len(i)) + i for i in seq])\n","\n","    def __len__(self): # datasetì˜ ì „ì²´ ê¸¸ì´ ë°˜í™˜\n","        return len(self.X)\n","\n","    def __getitem__(self, idx): # dataset ì ‘ê·¼\n","        X = self.X[idx]\n","        label = self.label[idx]\n","        return X, label"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"IyFqR5byD-NF","tags":[]},"outputs":[],"source":["def cleaning_text(text):\n","    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # íŠ¹ìˆ˜ë¬¸ì ë¥¼ ëª¨ë‘ ì§€ìš°ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n","    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break spaceë¥¼ unicode ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜\n","    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode ë¹ˆì¹¸ì„ ë¹ˆì¹¸ìœ¼ë¡œ ë³€í™˜\n","    return cleaned_text\n","\n","data = list(map(cleaning_text, data))\n","tokenizer = get_tokenizer(\"basic_english\")\n","vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, data))\n","vocab.insert_token('<pad>',0)\n","max_len = 20"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"_KqafQoED-NF","tags":[]},"outputs":[],"source":["# train set, validation set, test setìœ¼ë¡œ data setì„ ë‚˜ëˆ•ë‹ˆë‹¤. 8 : 1 : 1 ì˜ ë¹„ìœ¨ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n","train, test = train_test_split(data, test_size = .2, random_state = 42)\n","val, test = train_test_split(test, test_size = .5, random_state = 42)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ZiBdur8sD-NF","outputId":"8fe72ec9-254a-4f73-ff86-47715c546c29","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train ê°œìˆ˜:  5206\n","Validation ê°œìˆ˜:  651\n","Test ê°œìˆ˜:  651\n"]}],"source":["print(\"Train ê°œìˆ˜: \", len(train))\n","print(\"Validation ê°œìˆ˜: \", len(val))\n","print(\"Test ê°œìˆ˜: \", len(test))"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Mg5ThrQnD-NG","tags":[]},"outputs":[],"source":["train_dataset = CustomDataset(train, vocab, tokenizer, max_len)\n","valid_dataset = CustomDataset(val, vocab, tokenizer, max_len)\n","test_dataset = CustomDataset(test, vocab, tokenizer, max_len)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"STAxoDp6D-NG","tags":[]},"outputs":[],"source":["batch_size = 32\n","\n","train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n","test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"]},{"cell_type":"markdown","metadata":{"id":"DAV7ClRZD-NG"},"source":["## 2. Next word prediction ëª¨ë¸ êµ¬í˜„\n","\n","```\n","ğŸ’¡ ëª©ì°¨ ê°œìš”: Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•˜ê³ , ì´ë¥¼ í•™ìŠµí•˜ì—¬ ë´…ë‹ˆë‹¤.\n","```\n","\n","- 2-1. Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬í˜„\n","- 2-2. ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ "]},{"cell_type":"markdown","metadata":{"id":"Ld9wg6caD-NG"},"source":["### 2-1 Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬ì¶•\n","\n","> Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ì„ ì§ì ‘ êµ¬ì¶•í•´ë´…ë‹ˆë‹¤.\n"]},{"cell_type":"markdown","metadata":{"id":"gqBFtI8iD-NG"},"source":["#### ğŸ“ ì„¤ëª…: Next word predictionì„ ìœ„í•œ DNN ëª¨ë¸ êµ¬ì¶•\n","* DNN êµ¬í˜„ (2)ì—ì„œ í•™ìŠµí•˜ì˜€ë˜, DNN ëª¨ë¸ì„ ê¸°ë°˜ì— `nn.Embedding`ì„ ì¶”ê°€í•˜ì—¬ next word predictionì„ í•˜ê¸° ìœ„í•œ DNN ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n","* Embeddingì´ë€?\n","  * í…ìŠ¤íŠ¸ë‚˜ ë²”ì£¼í˜• ë°ì´í„°ì™€ ê°™ì´ ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ìš´ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n","  * ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì €ì°¨ì›ì˜ ë²¡í„° ê³µê°„ì— í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë‹¨ì–´, ë¬¸ì¥, ë²”ì£¼í˜• ë³€ìˆ˜ ë“±ì„ ê³ ì •ëœ ê¸¸ì´ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë§¤í•‘í•˜ì—¬ í‘œí˜„í•©ë‹ˆë‹¤.\n","* `nn.Embedding`\n","  * num_embedding : embeddingí•  inputê°’ì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ìì—°ì–´ì²˜ë¦¬ì—ì„  ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°ì™€ ë™ì¼í•©ë‹ˆë‹¤.\n","  * embedding_dim : embedding ë²¡í„°ì˜ ì°¨ì›ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n","  \n","ğŸ“š ì°¸ê³ í• ë§Œí•œ ìë£Œ:\n","* [torch.nn.Embedding - Pytorch ê³µì‹ íŠœí† ë¦¬ì–¼](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","* [Embedding ì„¤ëª…](https://wikidocs.net/64779)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"so6MkCyYD-NG","tags":[]},"outputs":[],"source":["class NextWordPredictionModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dims, hidden_dims, num_classes, dropout_ratio, set_super):\n","        if set_super:\n","            super().__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dims, padding_idx = 0) # padding index ì„¤ì • => gradient ê³„ì‚°ì—ì„œ ì œì™¸\n","        self.hidden_dims = hidden_dims\n","        self.layers = nn.ModuleList()\n","        self.num_classes = num_classes\n","        for i in range(len(self.hidden_dims) - 1):\n","            self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))\n","\n","            self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1]))\n","\n","            self.layers.append(nn.ReLU())\n","\n","            self.layers.append(nn.Dropout(dropout_ratio))\n","\n","        self.classifier = nn.Linear(self.hidden_dims[-1], self.num_classes)\n","        self.softmax = nn.LogSoftmax(dim = 1)\n","\n","    def forward(self, x):\n","        '''\n","        INPUT:\n","            x: [batch_size, sequence_len] # padding ì œì™¸\n","        OUTPUT:\n","            output : [batch_size, vocab_size]\n","        '''\n","        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n","        x = torch.sum(x, dim=1) # [batch_size, embedding_dim] ê° ë¬¸ì¥ì— ëŒ€í•´ ì„ë² ë”©ëœ ë‹¨ì–´ë“¤ì„ í•©ì³ì„œ, í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n","        for layer in self.layers:\n","            x = layer(x)\n","\n","        output = self.classifier(x) # [batch_size, num_classes]\n","        output = self.softmax(output) # [batch_size, num_classes]\n","        return output\n","\n","    def count_parameters(self):\n","        return sum(p.numel() for p in self.parameters() if p.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"vV45NcTRD-NH"},"source":["### 2-2 ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ \n","\n","> Next word prediction ëª¨ë¸ì„ ì§ì ‘ í•™ìŠµí•˜ê³ , textë¥¼ ì§ì ‘ ë„£ì–´ next word predictionì„ ì§ì ‘ ìˆ˜í–‰í•´ë´…ë‹ˆë‹¤.\n"]},{"cell_type":"markdown","metadata":{"id":"eDt45BiTD-NH"},"source":["#### ğŸ“ ì„¤ëª…: Next word prediction í•™ìŠµí•˜ê¸°\n","* DNN ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì •í•´ì¤ë‹ˆë‹¤.\n","* embedding layerì™€ fully connected layerì˜ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ hidden dimension ë¦¬ìŠ¤íŠ¸ êµ¬ì„± ì‹œ, embedding dimensionì„ ì²«ë²ˆì§¸ ê°’ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n","* ì˜ˆì¸¡í•˜ë ¤ëŠ” labelì˜ ê°œìˆ˜ëŠ” ë‹¨ì–´ ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜ì™€ ë™ì¼í•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"TxGR6EjKD-NH","tags":[]},"outputs":[],"source":["# training ì½”ë“œ, evaluation ì½”ë“œ, training loop ì½”ë“œ\n","def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n","    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n","    train_loss = 0.0\n","    train_accuracy = 0\n","\n","    tbar = tqdm(dataloader)\n","    for texts, labels in tbar:\n","        texts = texts.to(device)\n","        labels = labels.to(device)\n","\n","        # ìˆœì „íŒŒ\n","        outputs = model(texts)\n","\n","        loss = criterion(outputs, labels)\n","\n","        # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°\n","        train_loss += loss.item()\n","        # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜\n","        _, predicted = torch.max(outputs, dim=1)\n","\n","\n","        train_accuracy += (predicted == labels).sum().item()\n","\n","        # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •\n","        tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n","\n","    # ì—í­ë³„ í•™ìŠµ ê²°ê³¼ ì¶œë ¥\n","    train_loss = train_loss / len(dataloader)\n","    train_accuracy = train_accuracy / len(train_dataset)\n","\n","    return model, train_loss, train_accuracy\n","\n","def evaluation(model, dataloader, val_dataset, criterion, device, epoch, num_epochs):\n","    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n","    valid_loss = 0.0\n","    valid_accuracy = 0\n","\n","    with torch.no_grad(): # modelì˜ ì—…ë°ì´íŠ¸ ë§‰ê¸°\n","        tbar = tqdm(dataloader)\n","        for texts, labels in tbar:\n","            texts = texts.to(device)\n","            labels = labels.to(device)\n","\n","            # ìˆœì „íŒŒ\n","            outputs = model(texts)\n","            loss = criterion(outputs, labels)\n","\n","            # ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°\n","            valid_loss += loss.item()\n","            # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜\n","            _, predicted = torch.max(outputs, 1)\n","            # _, true_labels = torch.max(labels, dim=1)\n","            valid_accuracy += (predicted == labels).sum().item()\n","\n","\n","            # tqdmì˜ ì§„í–‰ë°”ì— í‘œì‹œë  ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •\n","            tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n","\n","    valid_loss = valid_loss / len(dataloader)\n","    valid_accuracy = valid_accuracy / len(val_dataset)\n","\n","    return model, valid_loss, valid_accuracy\n","\n","\n","def training_loop(model, train_dataloader, valid_dataloader, train_dataset, val_dataset, criterion, optimizer, device, num_epochs, patience, model_name):\n","    best_valid_loss = float('inf')  # ê°€ì¥ ì¢‹ì€ validation lossë¥¼ ì €ì¥\n","    early_stop_counter = 0  # ì¹´ìš´í„°\n","    valid_max_accuracy = -1\n","\n","    for epoch in range(num_epochs):\n","        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n","        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, val_dataset, criterion, device, epoch, num_epochs)\n","\n","        if valid_accuracy > valid_max_accuracy:\n","            valid_max_accuracy = valid_accuracy\n","\n","        # validation lossê°€ ê°ì†Œí•˜ë©´ ëª¨ë¸ ì €ì¥ ë° ì¹´ìš´í„° ë¦¬ì…‹\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n","            early_stop_counter = 0\n","\n","        # validation lossê°€ ì¦ê°€í•˜ê±°ë‚˜ ê°™ìœ¼ë©´ ì¹´ìš´í„° ì¦ê°€\n","        else:\n","            early_stop_counter += 1\n","\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n","\n","        # ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°ê°€ ì„¤ì •í•œ patienceë¥¼ ì´ˆê³¼í•˜ë©´ í•™ìŠµ ì¢…ë£Œ\n","        if early_stop_counter >= patience:\n","            print(\"Early stopping\")\n","            break\n","\n","    return model, valid_max_accuracy"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"referenced_widgets":["71f0de6186ef4bcdbc72758e46fe8938","dceb9c596e5c496aa5859e86bc68af23","6ffa288ecd0044c8a09727c171f8f88d","1cd671b15ecd42ee991ebb02fedc0393","7cf1826bb9844825a53ef89121fa6a83","a31ea30e437d41b88fc90dbfd0259713","c5a607bcabc74ec99f648402fedaa615","712d7f6d2a3240b8a8b6743576d022cb","1462f4bbc7cb4d74a22a4a884419f6f1","68a08ddd157441fba42e017b30e604df","02ac2be4562b428499c70bc05d5030f3","3dd951f27a404f3dbfb1fd51afdac352","ab16ed0ddfdb4b51ab38c14f51c40c27","c9312430adbc4c15adbea6418e7cc758","5d8f08d1cf9c44fdbebd6d03551a7008","5e17a25efcc34e47af40696309570663","fc8d7429e50b4682923e8c5991ba3a4e","eb426cd6107a4ab3844c7d2e36b08320"]},"id":"fSdBdm4JD-NH","outputId":"a2c30596-07b0-4da1-fba5-88b7d150c05b","tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62dd65c938e44f6fa03419bead9df267","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bde2b8a46ebb451f95f8c38a67127aef","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Train Loss: 7.3743, Train Accuracy: 0.0633 Valid Loss: 7.2358, Valid Accuracy: 0.0671\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85495051c6bc48d28ea91cebb3950725","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d8a23d753bd4ce081482edc3a845b7e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [2/100], Train Loss: 6.7342, Train Accuracy: 0.0759 Valid Loss: 7.2108, Valid Accuracy: 0.0779\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66b7809542984094beb5582e91d346ce","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea29e9f93b524c899cb79a2e57ed7c0a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [3/100], Train Loss: 6.3774, Train Accuracy: 0.0843 Valid Loss: 7.3121, Valid Accuracy: 0.0813\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc7e98a23bf04007a441f2f80d16c48c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcb67fd309b34e0abce6b14ac0ad06e2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [4/100], Train Loss: 6.0696, Train Accuracy: 0.0944 Valid Loss: 7.4566, Valid Accuracy: 0.0880\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d766604000c4b86ba5774405a4e8ad5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1159 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1efbd6d95fd54d6490728fc4db52cc77","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/149 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [5/100], Train Loss: 5.8033, Train Accuracy: 0.1034 Valid Loss: 7.6273, Valid Accuracy: 0.0935\n","Early stopping\n","Valid max accuracy :  0.09349936682144365\n"]}],"source":["lr = 1e-3\n","vocab_size = len(vocab.get_stoi())\n","embedding_dims = 512\n","hidden_dims = [embedding_dims, embedding_dims*4, embedding_dims*2, embedding_dims]\n","model = NextWordPredictionModel(vocab_size = vocab_size, embedding_dims = embedding_dims, hidden_dims = hidden_dims, num_classes = vocab_size, \\\n","            dropout_ratio = 0.2, set_super = True).to(device)\n","\n","num_epochs = 100\n","patience = 3\n","model_name = 'next'\n","\n","optimizer = optim.Adam(model.parameters(), lr = lr)\n","criterion = nn.NLLLoss(ignore_index=0) # padding í•œ ë¶€ë¶„ ì œì™¸\n","model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, train_dataset, valid_dataset, criterion, optimizer, device, num_epochs, patience, model_name)\n","print('Valid max accuracy : ', valid_max_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"70lYJ9aED-NH"},"source":["#### ğŸ“ ì„¤ëª…: Next word prediction í‰ê°€í•˜ê¸°\n","* í•™ìŠµí•œ DNN ëª¨ë¸ì„ accuracy scoreë¡œ í‰ê°€í•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"referenced_widgets":["76532b6300d844368e015b4f95d4561d","53224ea7733a497cbcb5da9be2817102"]},"id":"jK4sD9h2D-NH","outputId":"e8bd5426-2cd8-4059-80a6-797bd834791c","tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28b1f49da9cb4d63ab3f495c60b2471f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Next word prediction DNN model accuracy :  0.07480314960629922\n"]}],"source":["model.load_state_dict(torch.load(\"./model_next.pt\")) # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n","model = model.to(device)\n","model.eval()\n","total_labels = []\n","total_preds = []\n","with torch.no_grad():\n","    for texts, labels in tqdm(test_dataloader):\n","        texts = texts.to(device)\n","        labels = labels\n","\n","        outputs = model(texts)\n","        # torch.maxì—ì„œ dim ì¸ìì— ê°’ì„ ì¶”ê°€í•  ê²½ìš°, í•´ë‹¹ dimensionì—ì„œ ìµœëŒ“ê°’ê³¼ ìµœëŒ“ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        total_preds.extend(predicted.detach().cpu().tolist())\n","        total_labels.extend(labels.tolist())\n","\n","total_preds = np.array(total_preds)\n","total_labels = np.array(total_labels)\n","nwp_dnn_acc = accuracy_score(total_labels, total_preds) # ì •í™•ë„ ê³„ì‚°\n","print(\"Next word prediction DNN model accuracy : \", nwp_dnn_acc)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1692592994519,"user":{"displayName":"Eddie(ê¹€ìœ¤ê¸°)","userId":"11850705511374304262"},"user_tz":-540},"id":"OFsMBAeBZ1q0","outputId":"d2edb1d4-2a3d-406e-88d5-b12c71d13c42"},"outputs":[{"name":"stdout","output_type":"stream","text":["8618\n"]}],"source":["print(vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"c1HHIbXrp5pZ"},"source":["## Required Package\n","\n","> torch == 2.0.1\n","\n","> torchvision == 0.15.2\n","\n","> sklearn == 1.3.0\n","\n","> torchtext == 0.15.2"]},{"cell_type":"markdown","metadata":{"id":"k6fnxdyLp2Qk"},"source":["## ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤\n","\n","ì €ì‘ê¶Œ : <font color='blue'> <b> Â©2023 by Upstage X fastcampus Co., Ltd. All rights reserved.</font></b>\n","\n","<font color='red'><b>WARNING</font> : ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì—…ìŠ¤í…Œì´ì§€ ë° íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤. </b>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWBuGG4vhz1W"},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
